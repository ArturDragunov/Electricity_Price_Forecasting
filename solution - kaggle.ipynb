{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import utils as ut\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to have interactive html\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "df_consumption = pd.read_csv('consumption.csv')\n",
    "df_costs = pd.read_csv('costs.csv')\n",
    "df_net_imports = pd.read_csv('net_imports.csv')\n",
    "df_price = pd.read_csv('price.csv')\n",
    "df_production = pd.read_csv('production.csv')\n",
    "df_weather = pd.read_csv('weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "df_merge = df_price.merge(df_weather, on='datetime',how='left')\n",
    "df_merge = df_merge.merge(df_production,on='datetime',how='left')\n",
    "df_merge = df_merge.merge(df_net_imports,left_on='datetime',right_on='Date',how='left')\n",
    "df_merge = df_merge.merge(df_consumption,on='datetime',how='left')\n",
    "df_merge['date'] = pd.to_datetime(df_merge['datetime']).dt.date\n",
    "df_costs['date'] = pd.to_datetime(df_costs['date']).dt.date\n",
    "df_merge = df_merge.merge(df_costs,on='date',how='left')\n",
    "df_merge.drop(columns=['Date','date'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Spark Spread\n",
    "**Definition**: Measures profitability of electricity generated from natural gas after carbon costs.\n",
    "\n",
    "$$\n",
    "\\text{Clean Spark Spread} = (\\text{Electricity Price}) - (\\text{Natural Gas Price} \\times \\text{Heat Rate}) - (\\text{Carbon Price} \\times \\text{Emission Factor})\n",
    "$$\n",
    "\n",
    "### Clean Dark Spread\n",
    "**Definition**: Measures profitability of electricity generated from coal after carbon costs.\n",
    "\n",
    "$$\n",
    "\\text{Clean Dark Spread} = (\\text{Electricity Price}) - (\\text{Coal Price} \\times \\text{Heat Rate}) - (\\text{Carbon Price} \\times \\text{Emission Factor})\n",
    "$$\n",
    "\n",
    "Where \n",
    "- Electricity Price: The market price of electricity.\n",
    "- Coal/Gas Price: The price of coal/gas used as fuel.\n",
    "- Heat Rate: The efficiency of the power plant, typically expressed in MWh per unit of fuel. A lower heat rate indicates higher efficiency.\n",
    "- Carbon Price: The cost of carbon emissions per ton of CO₂.\n",
    "- Emission Factor: The amount of CO₂ emitted per unit of coal consumed (typically in tons of CO₂ per MWh).\n",
    "\n",
    "If Clean Spark Spread is significantly higher than Clean Dark Spread, then gas plants have advantage over coal plants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Dataset Analysis\n",
    "\n",
    "Before we go over the report, it's essential to notice that Clean Dark Spread (CDS) and Clean Spark Spread (CSS) are in daily format while other variables are in hourly granularity. It means that after merging, each daily value is repeated 24 times (24 hours per day).\n",
    "\n",
    "Based on the profile report, we can notice the following things:\n",
    "\n",
    "## Correlations\n",
    "\n",
    "- There are variable pairs with relatively high linear correlations. E.g. CDS and HardCoal or CSS and Natural Gas. These correlations are expected to be due to the nature of the CDS and CSS explained before.\n",
    "- Net Imports are highly negatively correlated with Wind behaviour. Germany's electricity generation relies heavily on renewables, especially wind. Thus, increased wind generation creates excess supply which Germany can export. These wind fluctuations could also affect price negative and positive spikes.\n",
    "- The negative correlation of Wind Onshore (land generated) and Offshore (off-land generated) with gas-fired power is understandable. Wind has low systematic price, especially in comparison with Gas. So, with increase of cheap electricity from wind turbines, the profit margin of gas-fired power decreases.\n",
    "- Consumption has positive correlation with gas and coal production, as with increase of consumption, plants start burning more gas and coal to match the demand.\n",
    "- Interesting that the price has high correlation with Coal and CDS but not as high correlation with gas. Probably because coal has a much higher portion of generated electricity in German market (https://en.wikipedia.org/wiki/Electricity_sector_in_Germany).\n",
    "- In relatively high correlations we can also notice such pairs as Temperature and Solar (with increase of sun, temperature also rises), Temperature and Dam and Ror (run-of-river), probably because of increased temperature there is melting snow, more rains than during winter and thus hydro generation grows.\n",
    "\n",
    "## Variable Statistics\n",
    "\n",
    "### Price:\n",
    "- Highly volatile, with a range from -90.01 to 700 €/MWh\n",
    "- Right-skewed distribution (mean 61.94, median 44.22)\n",
    "- 2.1% of prices are negative, indicating occasional oversupply\n",
    "\n",
    "### Wind Power (Onshore and Offshore):\n",
    "- Onshore wind has a much higher mean production (11,299 MW) compared to offshore (2,730 MW) meaning that there are much more land turbines than turbines based on water.\n",
    "\n",
    "### Solar:\n",
    "- 42.7% of values are zero due to nighttime hours when it's not shining.\n",
    "\n",
    "### Clean Spark Spread and Clean Dark Spread:\n",
    "- Both datasets start from 2019.\n",
    "- Clean Spark Spread is mostly negative (mean -12.45), suggesting gas plants often operate at a loss\n",
    "- Clean Dark Spread is also mostly negative but less so (mean -5.67), indicating coal plants might be more competitive. Which could be surprising as gas plants should be more flexible at reacting to demand changes than coal. Still, it could be caused by the increased gas prices in 2021-2022.\n",
    "\n",
    "### Net Import:\n",
    "- Mean of -3,294 MW indicates Germany is a net exporter on average\n",
    "- High variability, ranging from -17,807 MW (export) to 13,805 MW (import)\n",
    "\n",
    "In the following cells we will see that volatility of Price, Clean Spark Spread and Clean Dark Spread is extremely affected from mid-2021 and up to the limit of the dataset due to Energy Crisis and War in Ukraine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile1 = ProfileReport(df_merge, title=\"Power Data\", explorative=True)\n",
    "\n",
    "profile1.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Feature Selection\n",
    "From the report we also see that there are missing values in Wind. Let's have a look to these cases. There're 2 days where data is completely empty: 2018-12-31 and 2019-11-06. These days we will impute based on the previous day information. So, e.g. 2018-12-31 1:00 will receive data from 2018-12-30 1:00. 2019-11-07 00:00 will be backward imputed with 2019-11-07 01:00, the rest will be forward imputed. We assume that the nearest neighbors should have the best information about missing values. In addition, wind should also have seasonal patterns where usually it's lower during nights and higher during the day, thus we are imputing values based on previous day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.missing_rows_nan(df_merge,'WIND').datetime.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Fill WIND values for 2018-12-31 using 2018-12-30 data\n",
    "ut.fill_wind_from_previous_day(df_merge, '2018-12-31', '2018-12-30')\n",
    "\n",
    "# Fill WIND values for 2019-11-06 using 2019-11-05 data\n",
    "ut.fill_wind_from_previous_day(df_merge, '2019-11-06', '2019-11-05')\n",
    "\n",
    "# Fill 2019-11-07 00:00 using 2019-11-07 01:00\n",
    "df_merge['datetime'] = pd.to_datetime(df_merge['datetime'])\n",
    "backfill_mask = (df_merge['datetime'] == pd.to_datetime('2019-11-07 00:00:00'))\n",
    "impute_value = df_merge.loc[df_merge['datetime'] == pd.to_datetime('2019-11-07 01:00:00'), 'WIND'].values\n",
    "if len(impute_value) > 0:\n",
    "    df_merge.loc[backfill_mask, 'WIND'] = impute_value[0]\n",
    "\n",
    "# Forward fill the remaining missing WIND values\n",
    "df_merge['WIND'].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our time series and see their trajectories.\n",
    "The task of this case study is to predict May and June 2022 of hourly da_prices, and the last data available is April 2022. da_prices were very stable until mid-2021 where they started growing and got into the highest volatility in Dec 2021-March 2022. April 2022 demonstrates a slight decrease in overall prices and the volatility suggesting that May and June 2022 could either stay at the same level as the second half of April or continue the slow down trend.\n",
    "\n",
    "Variables Temperature, Wind, Dam, Solar, Ror, Wind OnShore and Offshore, Net Import and Consumption do not demonstrate and extreme spikes as these variables are not affected by geopolitics. So, we could for instance impute May and June values by using values of these variables in May and June from the past. We don't see any need in further transformations for these variables.\n",
    "\n",
    "Nuclear shows a step-wise behaviour where January is usually the month of decrease in nuclear power production for a following year. We decided to use Nuclear production not in raw but after transformations.\n",
    "\n",
    "CSS and CDS demonstrate similar volatility patterns, where these variables were stable until mid-2021 and after mid-2021 they increased their volatility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = importlib.reload(ut)\n",
    "df_merge.rename(columns={'datetime':'timestamp_rt'},inplace=True)\n",
    "df_merge['timestamp_rt'] = pd.to_datetime(df_merge['timestamp_rt'])\n",
    "column_list = ['price', 'TEMPERATURE', 'WIND', 'Dam', 'Solar',\n",
    "       'NaturalGas', 'Ror', 'HardCoal', 'Nuclear', 'WindOnshore',\n",
    "       'WindOffshore', 'NetImport', 'consumption', 'Clean Spark Spread',\n",
    "       'Clean Dark Spread']\n",
    "df_merge['node'] = 'Germany'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.create_plot(df_merge,targets=column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing\n",
    "### Step 1. Consistent Variables \n",
    "\n",
    "Let's impute variables using their past observations. These variables had systematic seasonal trend, so we assume that imputation from the previous year could be our best assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.set_index(['timestamp_rt', 'node'], inplace=True)\n",
    "\n",
    "# List of variables to impute\n",
    "variables_to_impute = ['TEMPERATURE', 'WIND', 'Dam', 'Solar', 'NaturalGas', 'Ror', 'HardCoal', 'WindOnshore', 'WindOffshore', 'NetImport', 'consumption']\n",
    "\n",
    "# Prepare the new data\n",
    "new_data = ut.prepare_new_data(df_merge, variables_to_impute)\n",
    "\n",
    "# Concatenate the new data to the original dataframe\n",
    "df_merge = pd.concat([df_merge, new_data])\n",
    "\n",
    "# Sort the index to ensure it's in chronological order\n",
    "df_merge = df_merge.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values are correctly imputed\n",
    "print('2021-05-01 00:00:00')\n",
    "df_merge.loc['2021-05-01 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2022-05-01 00:00:00')\n",
    "print('values are correctly imputed')\n",
    "df_merge.loc['2022-05-01 00:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Nuclear Imputation\n",
    "\n",
    "Step-wise Decrease in Nuclear: The Nuclear production exhibits a step-wise decrease during Januarys. It's visible that the last decrease happened in January 2022. \n",
    "\n",
    "The same way, as for other regressors, we will keep the logic assuming that data from the previous year (May-June 2021) is representative and can be used for imputing missing values for May-June 2022. However, we are not imputing the raw nuclear values but a percentage difference from the reference month - Median of January. We calculate Median January performance. Then, we calculate percentage changes with respect to the median. We then use percentages of May-June 2021 in May-June 2022 and multiply them by a reference median from January 2022. In other words, we assume that the production level is stable year to year but the nominal values are simply shifted down.\n",
    "\n",
    "So, as an example, if a median production in January 2021 was 100MWh, and in May 2021 we see 120MWh, then it's 120%, which we move to May 2022 and multiply by the median of January 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the median of January for Nuclear\n",
    "def get_january_median(df, year):\n",
    "    january_data = df.loc[(slice(f'{year}-01-01', f'{year}-01-31'), slice(None)), 'Nuclear']\n",
    "    return january_data.median()\n",
    "\n",
    "# Calculate the median January production for each year\n",
    "january_medians = {year: get_january_median(df_merge, year) for year in range(2018, 2023)}\n",
    "\n",
    "# Function to calculate the percentage of January median for each data point\n",
    "def calculate_percentage(value, year):\n",
    "    return value / january_medians[year] * 100\n",
    "\n",
    "# Calculate the percentage values for Nuclear\n",
    "df_merge['Nuclear_Percentage'] = df_merge.apply(lambda row: calculate_percentage(row['Nuclear'], row.name[0].year), axis=1)\n",
    "\n",
    "# Use the percentage values from May-June 2021 to impute May-June 2022\n",
    "may_june_2021 = df_merge.loc[(slice('2021-05-01', '2021-06-30'), slice(None)), 'Nuclear_Percentage']\n",
    "may_june_2022 = may_june_2021.copy()\n",
    "may_june_2022.index = pd.MultiIndex.from_arrays([may_june_2022.index.get_level_values(0) + pd.DateOffset(years=1), may_june_2022.index.get_level_values(1)])\n",
    "\n",
    "df_merge.loc[(slice('2022-05-01', '2022-06-30'), slice(None)), 'Nuclear_Percentage'] = may_june_2022\n",
    "\n",
    "# Convert the percentage back to absolute values for 2022\n",
    "df_merge.loc[(slice('2022-05-01', '2022-06-30'), slice(None)), 'Nuclear'] = df_merge.loc[(slice('2022-05-01', '2022-06-30'), slice(None)), 'Nuclear_Percentage'] * january_medians[2022] / 100\n",
    "df_merge.drop(columns=['Nuclear_Percentage'],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. CSS and CDS\n",
    "\n",
    "We need to deal with CSS and CDS. We can't simply impute them from 2021. These ratios are based on prices and thus are also extremely volatile. We tried different approaches and calculated different ratios and we believe that the best way would be to use rolling MA and STD for the difference between Price and CDS/CSS. This way we isolate:\n",
    "$$\n",
    "(\\text{Commodity Price} \\times \\text{Heat Rate}) - (\\text{Carbon Price} \\times \\text{Emission Factor}).\n",
    "$$\n",
    "\n",
    "Our strategy is to fit ARIMA models on historical data until the beginning of the changes in behavior Y2021. We then predict values for 2 months (data is in daily format, so we do 61 predictions). We subtract forecasted medians from values and add medians of April 2022. So, we keep historical level fluctuations but we shift the forecasts on the level of April 2022. \n",
    "\n",
    "We do it based on our assumption that May and June 2022 should keep similar historical movements but be on the similar levels as for April 2022. April demonstrates decreases in volatilities compared to March 2022, so it could be a good month for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merge.copy()\n",
    "\n",
    "# first attempts\n",
    "df['CDS_to_Price_Ratio'] = df['Clean Dark Spread'] / df['price']\n",
    "df['CSS_to_Price_Ratio'] = df['Clean Spark Spread'] / df['price']\n",
    "df['Normalized_Spread_Difference'] = (df['Clean Dark Spread'] - df['Clean Spark Spread']) / df['price']\n",
    "window_2w = 24 * 7 * 2  # 2 week\n",
    "window = 24 * 7  # 1 week\n",
    "df['CDS_Rolling_Mean'] = df['Clean Dark Spread'].rolling(window_2w).mean()\n",
    "df['CSS_Rolling_Mean'] = df['Clean Spark Spread'].rolling(window_2w).mean()\n",
    "df['CDS_Rolling_Std1w'] = df['Clean Dark Spread'].rolling(window).std()\n",
    "df['CSS_Rolling_Std1w'] = df['Clean Spark Spread'].rolling(window).std()\n",
    "df['CDS_EMA'] = df['Clean Dark Spread'].ewm(span=24*7*2).mean()\n",
    "df['CSS_EMA'] = df['Clean Spark Spread'].ewm(span=24*7*2).mean()\n",
    "\n",
    "# Average Daily Price minus CDS/CSS\n",
    "df_daily_avg = df.groupby([pd.Grouper(level='timestamp_rt', freq='D'), 'node']).mean()\n",
    "\n",
    "# Calculate Price_minus_CDS and Price_minus_CSS\n",
    "df_daily_avg['Price_minus_CDS'] = df_daily_avg['price'] - df_daily_avg['Clean Dark Spread']\n",
    "df_daily_avg['Price_minus_CSS'] = df_daily_avg['price'] - df_daily_avg['Clean Spark Spread']\n",
    "df_daily_avg.reset_index(inplace=True)\n",
    "\n",
    "# Rename 'timestamp_rt' to 'date' for merging\n",
    "df_daily_avg.rename(columns={'timestamp_rt': 'date'}, inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df['date'] = pd.to_datetime(df['timestamp_rt']).dt.date\n",
    "df_daily_avg['date'] = pd.to_datetime(df_daily_avg['date']).dt.date\n",
    "\n",
    "# Merge the daily average back to the original DataFrame\n",
    "df = df.merge(df_daily_avg[['date', 'node', 'Price_minus_CDS', 'Price_minus_CSS']], on=['date', 'node'], how='left')\n",
    "\n",
    "# working options\n",
    "df['Price_minus_CDS_MA1w'] = df['Price_minus_CDS'].rolling(window).mean()\n",
    "df['Price_minus_CSS_MA1w'] = df['Price_minus_CSS'].rolling(window).mean()\n",
    "df['Price_minus_CDS_SD1w'] = df['Price_minus_CDS'].rolling(window).std()\n",
    "df['Price_minus_CSS_SD1w'] = df['Price_minus_CSS'].rolling(window).std()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = ['Price_minus_CSS','Price_minus_CDS','Price_minus_CDS_MA1w','Price_minus_CDS_SD1w','Price_minus_CSS_MA1w','Price_minus_CSS_SD1w']\n",
    "ut.create_plot(df.reset_index(),targets=list_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df_merge.reset_index(inplace=True)\n",
    "df_merge = df_merge.merge(df[['timestamp_rt', 'node','Price_minus_CDS_MA1w','Price_minus_CSS_MA1w', 'Price_minus_CDS_SD1w','Price_minus_CSS_SD1w']], on=['timestamp_rt', 'node'], how='left')\n",
    "df_merge.drop(columns=['Clean Spark Spread', 'Clean Dark Spread'],inplace=True) # we drop them as we won't use them in May/June 2022 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Define the variables\n",
    "variables = ['Price_minus_CDS_MA1w', 'Price_minus_CSS_MA1w', 'Price_minus_CDS_SD1w', \n",
    "             'Price_minus_CSS_SD1w']\n",
    "\n",
    "# Define the dates\n",
    "ma_train_end = '2021-01-31'\n",
    "std_train_end = '2021-10-31'\n",
    "forecast_start = '2022-05-01'\n",
    "forecast_end = '2022-06-30'\n",
    "\n",
    "# Create the 'date' column\n",
    "df_merge['date'] = pd.to_datetime(df_merge['timestamp_rt']).dt.date\n",
    "\n",
    "# Calculate the daily averages\n",
    "df_daily_avg = df_merge.groupby('date')[variables].mean().reset_index()\n",
    "\n",
    "# Process each variable one by one\n",
    "for var in variables:\n",
    "    if 'MA' in var:\n",
    "        train_end = ma_train_end\n",
    "    else:\n",
    "        train_end = std_train_end\n",
    "    \n",
    "    adjusted_forecast = ut.predict_and_adjust(df_daily_avg.set_index('date')[var], train_end, forecast_start, forecast_end)\n",
    "    \n",
    "    # Add the forecast to the dataframe\n",
    "    for date in adjusted_forecast.index:\n",
    "        df_merge.loc[df_merge['date'] == date.date(), var] = adjusted_forecast.loc[date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = ['Price_minus_CDS_MA1w','Price_minus_CDS_SD1w','Price_minus_CSS_MA1w','Price_minus_CSS_SD1w']\n",
    "ut.create_plot(df_merge,targets=list_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Inspection of DA prices\n",
    "\n",
    "We are visualizing weekly price profiles based on historical data up to 2021Y. We exclude more recent data because it's systematically different than the historical data and could corrupt the profiles.\n",
    "\n",
    "As expected, working weeks have very similar two hill movements of prices during the day, when the minimum plateau is during early morning hours before everyone starts waking up and preparing for work. The peak hours are at 8 and 9AM. This is usually the time for spikes where demand is higher than supply. Then we have a decrease after 8AM, one more uphill when people start finishing their work and one more downmovement as everyone goes to sleep. Saturday and Sunday have absolutely different profiles, as most of the people don't work on these days. \n",
    "\n",
    "It suggests us to have a separate variable for day of the week and hours.\n",
    "\n",
    "Unfortunately, as our task is to predict 2 months of prices during the shocking period, we can't use lags from e.g. last year prices because they are not relevant in 2022. What we can do is to enrich our X matrix with seasonal terms and try to model prices using our regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ut.plot_weekly_profile(df_merge, 'Germany')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation and Backtesting\n",
    "Let's now create additional features and prepare our matrix for regression. We use intraday, weekly and yearly fouerier terms and radial basis functions. We create categorical variables instead of dummies for hours, days of the week, months, years. We don't want to use dummies because we will be using lightgbm model, and it works fine with categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge, fourier_cols = ut.create_fourier_terms(df_merge)\n",
    "\n",
    "df_merge['hour'] = pd.Categorical(df_merge['timestamp_rt'].dt.hour)\n",
    "df_merge = ut.create_RBF_features(df_merge, alpha=1000, col='hour') # 1000 is based on my previous work-experience. Evaluated it empirically.\n",
    "df_merge['day_of_week'] = pd.Categorical(df_merge['timestamp_rt'].dt.dayofweek)\n",
    "df_merge['month'] = pd.Categorical(df_merge['timestamp_rt'].dt.month)\n",
    "df_merge['year'] = pd.Categorical(df_merge['timestamp_rt'].dt.year)\n",
    "\n",
    "df_merge.set_index('timestamp_rt',inplace=True)\n",
    "df_merge.drop(columns=['date','node'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are limited in resources, we won't be able to tune hyperparameters sufficiently. LGBM is well-known for its well-performing default hyperparameters. We will try to tune them a bit using a small set of hyperparameter combinations. We will compare the best performing models based on RMSE, and we will check trajectories of our best model. We will fit lgbm on 1 year of data, test it on 2 months, then refit using sliding window. We will then compare final results up to April 2022 and then use the tuned final model for May and June 2022 predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'n_estimators': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# Initialize the default parameters\n",
    "default_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    \"verbosity\": -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    \"num_threads\": 2,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "importlib.reload(ut)\n",
    "# Define the backtesting period\n",
    "predict_start = '2021-01-01'\n",
    "predict_end = '2022-05-01' # not included\n",
    "\n",
    "# Perform manual grid search\n",
    "grid_search_results = ut.manual_grid_search(df_merge, param_grid, default_params, predict_start, predict_end)\n",
    "\n",
    "# Print the results\n",
    "for result in grid_search_results:\n",
    "    print(f\"Params: {result['params']}, RMSE: {result['rmse']}\")\n",
    "\n",
    "# Find the best parameters\n",
    "best_result = min(grid_search_results, key=lambda x: x['rmse'])\n",
    "print(f\"Best parameters: {best_result['params']}, Best RMSE: {best_result['rmse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters\n",
    "best_result = min(grid_search_results, key=lambda x: x['rmse'])\n",
    "print(f\"Best parameters: {best_result['params']}, Best RMSE: {best_result['rmse']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refit the best model and see its trajectories and feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Backtest with best parameters\n",
    "best_params = best_result['params']\n",
    "rmse_best, results_best, gain_importances_best, split_importances_best, models_best = ut.backtest_model(df_merge, best_params, predict_start, predict_end)\n",
    "print('Best RMSE',rmse_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trajectories have a good seasonal power, they move similarly to actual values. Still, forecasts are often undervalued compared to actuals. But predictions perform pretty well during volatility times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_best_visual = results_best.reset_index()\n",
    "results_best_visual['node'] = 'Germany'\n",
    "ut.create_plot(results_best_visual,targets=['actual','predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "We now can evaluate feature importances. We will compare Shapley values with traditional methods based on gain and split importances.\n",
    "\n",
    "SHAP measures the impact of variables taking into account the interaction with other variables.\n",
    "\n",
    "Shapley values calculate the importance of a feature by comparing what a model predicts with and without the feature. \n",
    "\n",
    "However, since the order in which a model sees features can affect its predictions, this is done in every possible order,\n",
    "so that the features are fairly compared.\n",
    "\n",
    "This plot shows the 20 most important features of the best performing model. Features are ordered in descending order based on their importance. For each feature a distribution is plotted on how the train samples\n",
    "influence the model outcome. The color corresponds to the values of the feature - the brighter is the red color, the higher is the feature value. If the color is blue, the feature values are low. X axis of the plot corresponds to the values of the target.\n",
    "\n",
    "For Instance, HardCoal red values (high values of production generated from Hard Coal) correspond to high values for prices -> expected behavior. Coal is not a cheap resource compared to renewables, so it's profitable to burn it for electricity when prices are high. Another example, for instance, could be for wind -  high values of wind production correspond to low values for prices. Grey values for hour or month are due to the fact that these variables are categorical, so there's no high or low hour.\n",
    "\n",
    "We can see that our Price-CDS/CSS features are also performing well, which should make our lgbm model to keep the forecast values of May and June 2022 at similar levels as it's in April 2022.\n",
    "\n",
    "Shapley feature importances are more robust than standard ones based on split/gain importances. Still, it's always good to have a look to all three chart types and to see the matching variables. For instance, based on all three types of visualizations: \n",
    "- Naturalgas and Coal productions are one of the most important features for day-ahead price predictions. It's expected as these energy production resources take a big portion of all energy supply in Germany.\n",
    "- Seasonal features such as hours, fourier terms also play important role, as they help to model systematic seasonal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP importances for the last model\n",
    "ut.plot_shap_importances(models_best[-1], df_merge.drop(columns=['price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=ut.plot_importances(split_importances_best, gain_importances_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# May and June 2022 Prediction\n",
    "We finally fit our best performing model on last year data up to 2022-04-30 23:00, and predict 2 months of data up to 22-06-30 23:00. We then can visualize trajectories and make a qualitative evaluation whether our predictions match our assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Best hyperparameters\n",
    "best_params = {'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'boosting_type': 'gbdt', 'num_threads': 2, 'seed': 42, 'learning_rate': 0.1, 'num_leaves': 50, 'max_depth': 10, 'n_estimators': 500}\n",
    "\n",
    "# Define the training and prediction periods\n",
    "train_end_date = pd.to_datetime('2022-04-30 23:00:00')\n",
    "predict_end_date = pd.to_datetime('2022-07-01 00:00:00')\n",
    "\n",
    "# Fit the model and make predictions\n",
    "predictions_df, best_model = ut.fit_and_predict(df_merge, best_params, train_end_date, predict_end_date)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_df.to_csv('predictions_may_june_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_merge.merge(predictions_df, on='timestamp_rt', how='left')\n",
    "df_final['node']='Germany'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output respects our assumption that May and June 2022 might have slightly lower but very similar behavior as April 2022. We see that day-ahead prices are slightly shifted down but respect the overall levels of April 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.create_plot(df_final.reset_index(),targets=['price','predicted'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "profiling_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
